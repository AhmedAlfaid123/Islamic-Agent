# -*- coding: utf-8 -*-
"""Islamic Agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zr5n9ctSTgdSMFqZhfPk4PZ_e6FKqkRS

#**Ahmed Hassan Abbas**
"""

!pip install google-search-results

!pip install langchain langchain_community langchain_google_genai chromadb gradio

!unzip KOTOB.zip

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from google.colab import userdata
from langchain_community.vectorstores import Chroma
from langchain.schema import Document
import os
import json
import requests
import gradio as gr
import re

def normalize_arabic(text: str) -> str:
    if not text:
        return ""
    text = re.sub(r'[\u0610-\u061A\u064B-\u065F\u06D6-\u06ED]', '', text)
    text = text.replace('\u200f','').replace('\u200e','')
    text = text.strip()
    text = re.sub(r'\s+', ' ', text)
    return text

# ===================================
# 2) Loading data, metadata
# ===================================
documents = []
files_to_load = ['muslim.json', 'bukhari.json', 'malik.json', 'ahmed.json']

for file_name in files_to_load:
    file_path = os.path.join(file_name)
    try:
        if file_name.endswith('.json'):
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if 'hadiths' in data:
                for hadith in data['hadiths']:
                    if 'arabic' in hadith and hadith['arabic']:
                        content = normalize_arabic(hadith['arabic'])
                        metadata = {
                            "source": file_name,
                            "id": hadith.get('id'),
                            "bookId": hadith.get('bookId'),
                            "chapterId": hadith.get('chapterId')
                        }
                        documents.append(Document(page_content=content, metadata=metadata))
            print(f"Loaded from {file_name}")

        elif file_name.endswith('.txt'):
            loader = TextLoader(file_path)
            loaded_docs = loader.load()
            for doc in loaded_docs:
                doc.metadata["source"] = file_name
            documents.extend(loaded_docs)
            print(f"Loaded {len(loaded_docs)} from {file_name}")

        else:
            print(f"Skipping unsupported: {file_name}")
    except Exception as e:
        print(f"Error in {file_name}: {e}")

print(f"\n Total documents loaded: {len(documents)}")

# ===================================
# 3) Create  Embeddings , VectorStore
# ===================================
from langchain.embeddings import HuggingFaceEmbeddings

# embeddings = GoogleGenerativeAIEmbeddings(
#     model="models/embedding-001",
#     google_api_key=userdata.get('GEMINI_API_KEY')
# )

embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-base")


text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
split_documents = text_splitter.split_documents(documents)

print(f"Original docs: {len(documents)}")
print(f"Split docs: {len(split_documents)}")

vectorstore = Chroma.from_documents(
    documents=split_documents,
    embedding=embeddings,
    persist_directory="./chroma_db"
)
vectorstore.persist()

# ===================================
# 4) OpenRouter API connection
# ===================================
OPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')

def call_openrouter_api(prompt):
    response = requests.post(
        url="https://openrouter.ai/api/v1/chat/completions",
        headers={
            "Authorization": f"Bearer {OPENROUTER_API_KEY}",
            "Content-Type": "application/json"
        },
        json={
            "model": "openai/gpt-oss-120b",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.0,
            "max_tokens": 900
        }
    )
    data = response.json()
    try:
        return data['choices'][0]['message']['content']
    except:
        return response.json()

call_openrouter_api("Hello")

# ===================================
# 5) Re-ranking
# ===================================
def rerank_results(query, docs, top_k=5):
    scores = []
    for doc in docs:
        check_prompt = f"""السؤال: {query}
النص: {doc.page_content}

هل النص أعلاه يجيب على السؤال بشكل مباشر؟ أجب فقط بـ نعم أو لا.
"""
        ans = call_openrouter_api(check_prompt)
        score = 1 if "نعم" in ans else 0
        scores.append((doc, score))
    sorted_docs = sorted(scores, key=lambda x: x[1], reverse=True)
    return [doc for doc, _ in sorted_docs[:top_k]]

from langchain_community.utilities import GoogleSerperAPIWrapper
import os

SERPER_API_KEY = userdata.get('SERPER_API_KEY')

# ===================================================
# 6) Searching with filtering by metadata, Re-ranking
# ===================================================
def ask_question_with_retrieval(query, book_filter="All Books"):
    search_kwargs = {"k": 50}
    if book_filter != "All Books":
        search_kwargs["filter"] = {"source": book_filter}

    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs=search_kwargs)
    initial_docs = retriever.get_relevant_documents(query)

    top_docs = rerank_results(query, initial_docs, top_k=5)

    context = "\n".join([f"Document from Vectorstore:\n{doc.page_content}" for doc in top_docs])

    scraped_content = ""
    scraped_source_url = ""

    if SERPER_API_KEY:
        print("Performing web search to supplement retrieval...")
        try:
            search = GoogleSerperAPIWrapper(serper_api_key=SERPER_API_KEY)
            search_results = search.results(query)
            web_context = ""

            if 'organic' in search_results:
                for result in search_results['organic']:
                    if result.get('link') and "shamela.ws" in result['link']:
                        print(f"Found trusted source: {result['link']}. Attempting to scrape.")
                        scrape_url = result['link']
                        scrape_response = requests.post(
                            url="https://serper.dev/api/scrape",
                            headers={
                                "X-API-KEY": SERPER_API_KEY,
                                "Content-Type": "application/json"
                            },
                            json={"url": scrape_url}
                        )
                        if scrape_response.status_code == 200:
                            scrape_data = scrape_response.json()
                            if 'article' in scrape_data and scrape_data['article']:
                                scraped_content = normalize_arabic(scrape_data['article'])
                                scraped_source_url = scrape_url
                                print("Successfully scraped content from trusted source.")
                                break
                            else:
                                print("Scrape API returned no article content.")
                        else:
                            print(f"Scrape API failed with status code: {scrape_response.status_code}")
                            print(f"Scrape API response: {scrape_response.text}")

                    web_context += f"Web Search Result from {result.get('title')}:\n{result.get('snippet')}\n\n"

            if scraped_content:
                context = f"Scraped Content from {scraped_source_url}:\n{scraped_content}\n\n" + context
                print("Scraped content incorporated into context.")
            elif web_context:
                context = context + "\n\n" + web_context
                print("General web search results incorporated.")
            else:
                print("Web search returned no relevant results.")

        except Exception as e:
            print(f"Web search or scraping failed: {e}")
    else:
        print("SERPER_API_KEY not available, skipping web search and scraping.")


    prompt = f"""
أنت وكيل مُتخصِّص في علم الحديث النبوي ومصادر المعلومات العامة. مهمتك:
1) أجب على سؤال المستخدم بدقة وبوضوح باللغة العربية.
2) استعمل فقط المعلومات الواردة في المقاطع (context) التالية عند تكوين الإجابة. لا تختلق مصادر جديدة.
3) عندما تذكر معلومات من أي مقطع من Vectorstore ضع الاستشهاد بصيغة مربعات مثل: [المصدر | رقم الحديث | من اي باب  | حالة الحديث صحيح او حسن او ضعيف او مكذوب ].
4) عندما تذكر معلومات من أي مقطع من Web Search Result اذكر المصدر بصيغة مربعات مثل: [موقع: عنوان الموقع].
5) عندما تذكر معلومات من أي مقطع من Scraped Content اذكر المصدر بصيغة مربعات مثل: [المصدر المُستَخرَج: عنوان URL].
6) إذا لم تكفِ المقاطع (من Vectorstore أو Web Search أو Scraped Content) للإجابة بصراحة اعترف بنقص الدليل وقل (لا أمتلك دليلاً كافياً في المصادر المتاحة).
7) لا تُقدّم فتوى دينية جديدة — إن كان السؤال فقهيّا ادعُ المستخدم إلى الرجوع لعلماء متخصصين بعد عرض دلائل النص.
8) اذكر نص الحديث عربيًا (إن اقتبست مباشرة من المقاطع من Vectorstore أو Scraped Content إذا كان حديثًا) بين علامتي اقتباس مزدوجة "" ثم الاستشهاد المناسب.
9) النهاية: أدرج قائمة قصيرة "المصادر المستخدمة" بترتيب الاستشهادات، **مع التفرقة بين مصادر Vectorstore ومصادر Web Search ومصادر Scraped Content عن طريق تسمية كل قسم بوضوح (مثلاً: مصادر من قاعدة الأحاديث, مصادر من البحث عبر الويب, مصادر من المحتوى المُستَخرَج).**

الآن أدناه المقاطع (context) المستخرجة من كتب الأحاديث والبحث عبر الويب والمحتوى المُستَخرَج:
{context}

السؤال: {query}

أجب الآن باتباع التعليمات أعلاه.
"""
    answer = call_openrouter_api(prompt)
    return {"result": answer, "source_documents": top_docs, "scraped_source": scraped_source_url}

books_list = ["All Books"] + files_to_load

def chatbot_interface(user_message, selected_book, history):
    if not user_message.strip():
        return history + [["", "Please enter a question."]]

    resp = ask_question_with_retrieval(user_message, selected_book)
    answer = resp["result"]

    if "Web Search Result" in answer or (len(resp["source_documents"]) < 5 and SERPER_API_KEY):
         answer += "\n\n*(Note: Web search was used to supplement the response.)*"

    history = history + [[user_message, answer]]
    return history

with gr.Blocks() as iface:
    gr.Markdown("# Hadith Chatbot")

    with gr.Row():
        selected_book = gr.Dropdown(choices=books_list, value="All Books", label="Select Book")

    chatbot = gr.Chatbot(label="Conversation")
    msg = gr.Textbox(placeholder="Type your message here...", label="Your Message")
    clear = gr.Button("Clear Chat")

    msg.submit(chatbot_interface, [msg, selected_book, chatbot], chatbot)
    clear.click(lambda: None, None, chatbot)

iface.launch(inline=True)

#----------------------TESTING------------------------
test_query_2 = "ما هو حكم ترك الصلاة؟"
print(f"Testing query: {test_query_2}")
response_2 = ask_question_with_retrieval(test_query_2)

print("\nResponse from the chatbot:")
print(response_2['result'])
print("\nSource documents (from vectorstore):")
for i, doc in enumerate(response_2['source_documents']):
    print(f"Document {i+1}:")
    print(f"  Content: {doc.page_content}")
    print(f"  Source: {doc.metadata.get('source', 'N/A')}")
    print(f"  ID: {doc.metadata.get('id', 'N/A')}")
    print(f"  Book ID: {doc.metadata.get('bookId', 'N/A')}")
    print(f"  Chapter ID: {doc.metadata.get('chapterId', 'N/A')}")